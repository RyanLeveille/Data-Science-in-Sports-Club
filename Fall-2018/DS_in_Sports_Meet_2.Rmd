---
title: "DS in Sports, Meeting 2."
author: "Yo"
date: "September 13, 2018"
output:
html_document: default
pdf_document: default
---
  Evidently there's a fresh place (launched **Sep 5th, 2018**) to look for a variety of data: 
  https://toolbox.google.com/datasetsearch
  
  First, we install some $R$ libraries: besides the already used $rvest$ and $plotrix$, we will also employ $stringi$ library to help us with string formatting.
  

```{r, include = F}
knitr::opts_chunk$set(echo = TRUE, message = F, warning = F)
```

```{r}
library(rvest)  # Should be installed along with 'xml2'
library(plotrix) # Also install it please.
library(stringi) # Needed for certain string operations.
```

Below, we supply a url address with data of interest and read it via $read\_html()$ function.
```{r}
url_link <- "https://www.sports-reference.com/cfb/schools/houston/2017/gamelog/"
#url_link <- "https://www.sports-reference.com/cfb/players/ed-oliver-2.html"

# Read the HTML webpage for that link into an R object
url <- read_html(url_link) 
```

Last time we extracted the $Offense$ logs from such pages, and saved them to $Scraped\_Offense.txt$ file. Let's read it into an $offense\_data$ object:

```{r}
offense_data <- read.table("Scraped_Offense.txt")
```

Now, what about the $Defense$ logs though? The $Defense$ table is not being picked up by the $xml\_find\_all(url, "//table")$ command. As it turns out, the $Defense$ table is *commented out* in the page source code.

```{r, echo=FALSE, fig.cap="Offensive tables are NOT commented out.", out.width = '100%'}
knitr::include_graphics("/home/usdandres/Pictures/Offense_NOT_Commented_Out.png")
```





```{r, echo=FALSE, fig.cap="Defensive tables are commented out (for some reason).", out.width = '100%'}
knitr::include_graphics("/home/usdandres/Pictures/Defense_Commented_Out.png")
```

The following code creates a function $find\_extra\_table()$ to:

  - find the commented parts of page source code which contain 'table' element,
  
  - strip out the 'comment' aspects of the source code,
  
  - apply the same code we performed for the uncommented offense table before.

```{r, message=F, warning=F}
find_extra_table <- function(url_link){
  # Additional tables are within the comment tags -  between "<!--"  and  "-->"
  # Which is why your xpath is missing them.
  
  # Find all the commented nodes
  alt_tables <- xml_find_all(read_html(url_link),"//comment()")
  
  #Among commented nodes, find those containing '<table' reg. expression 
  #raw_parts <- as.character(alt_tables[grep("\\</?table", as.character(alt_tables))])
  raw_parts <- as.character(alt_tables[grep("<table", as.character(alt_tables))])
  
  # Remove the comment begin ("<!--") and end ("-->") tags
  strip_html <- stringi::stri_replace_all_regex(raw_parts, c("<\\!--","-->"),c("",""),
                                                vectorize_all = FALSE)
  
  # Apply what we did for UNcommented table before
  strip_html <- read_html(strip_html)
  table_two <- xml_find_all(strip_html, "//table") 
  class(table_two)
  
  # Obtain the actual contents of the table, wrapped into a LIST of DATA FRAMES
  table_two <- html_table(table_two)
  return(table_two)
}
```

Let's try this function for our Houston Cougars url. Appears to work just as it did for the offensive logs prior.
```{r}
find_extra_table(url_link)[[1]]
```

Now we just have to 

  - cycle through the 5 teams of interest (Houston, Rice, Texas Tech, Temple, Tulsa)
  
  - clean up the resulting table to turn it into a data frame of digestible format.

```{r}
defense_data <- NULL
team_names <- c('houston', 'rice', 'texas-tech','temple','tulsa')

for (i in 1:length(team_names)){
  url_link <- paste('https://www.sports-reference.com/cfb/schools/',team_names[i],'/2017/gamelog/',
                    sep="") 
  table_two <- find_extra_table(url_link)[[1]]
  
  # That can be fixed via hard-coding:
  tab.col.names <- table_two[1,]
  table_two <- table_two[-1,]
  colnames(table_two) <- tab.col.names
  rownames(table_two) <- c(1:nrow(table_two))
  
  # Also, we don't quite need the LAST ROW, which is just totals for the year:
  # we can always calculate those on our own.
  table_two <- table_two[-nrow(table_two),]
  
  # Function as numeric comes to rescue:
  table_two[,-c(2:5)] <- lapply(table_two[,-c(2:5)],as.numeric)
  table_two[,1] <- team_names[i]
  colnames(table_two)[1] <- "Team"
  
  defense_data <- rbind(defense_data, table_two)
}

dim(defense_data)

head(defense_data)
```

Just like we did with offensive logs, let's save those scraped tables into a $Scraped\_Defense.txt$ file:

```{r}
write.table(defense_data,"Scraped_Defense.txt")

# And just read it back into defense_data right away, 
# to make sure both offense_data and defense_data are of the same format.
defense_data <- read.table("Scraped_Defense.txt")  
```


## Data Wrangling: Working with Strings.

To finalize the **data wrangling** (<=> putting data into that **final format**, which we'll use for model fitting/analysis), we will convert the $Result$ string into:

- points **scored** (for the offense table)
     
- points **allowed** (for the defense table)


First, let's convert a single 'Result' value into points scored and allowed.

```{r}
offense_data$Result[1]
class(offense_data$Result[1]) # Factor variable can't be worked on in terms of extracting sub-strings => 
as.character(offense_data$Result[1]) # convert to character via 'as.character()' function.
```
 To split it into "points scored" & "points allowed", we use base R's $strsplit()$ function, which takes as arguments:
 
   - String to be split into parts,
   
   - Character according to which the string will be split.
```{r}
spl <- strsplit(as.character(offense_data$Result[1]), split=" ")
spl[[1]]
spl[[1]][2]

score <- spl[[1]][2]

score.no.parenth <- substr(score,
                           start=2,
                           stop=nchar(score)-1)

strsplit(score.no.parenth,
         split="-")

points.scored <- strsplit(score.no.parenth,split="-")[[1]][1]
points.allowed <- strsplit(score.no.parenth,split="-")[[1]][2]

as.numeric(points.scored)
as.numeric(points.allowed)
```

Now, to apply this to the whole $Result$ vector, we defer to $apply()$ function. We "simply"" combine **all of the above steps** into the **function argument** of $sapply()$:
```{r}
Points <- sapply(as.character(offense_data$Result),
       function(x){ 
         spl <- strsplit(as.character(x), split=" ");
         score <- spl[[1]][2]
         score.no.parenth <- substr(score,2,nchar(score)-1)
         points.scored <- strsplit(score.no.parenth,split="-")[[1]][1]
         points.allowed <- strsplit(score.no.parenth,split="-")[[1]][2]
         return(c(points.scored=as.numeric(points.scored),
                  points.allowed=as.numeric(points.allowed)))})

# Add a 'Points' column to both 
#   'offense_data' (for points scored), and
#   'defense_data' (for points allowed).
offense_data$Points <- Points[1,]
defense_data$Points <- Points[2,]
```


## Playing with Scraped Data.

Now that we obtain nicely formatted data frames for both the offensive and defensive logs, let's do some analysis. 

First, we could get averages that each team gained/allowed in various statistical categories. Function $tapply()$ allows us to achieve that:

```{r}
### Offense

attach(offense_data)
# Passing
tapply(Yds, Team, mean)
tapply(TD, Team, mean)
# Rushing
tapply(Yds.1, Team, mean)
tapply(TD.1, Team, mean)
# Turnovers
tapply(Fum, Team, mean)
tapply(Int, Team, mean)
tapply(Tot.1, Team, mean)

detach(offense_data)


### Defense

attach(defense_data)

# Passing
tapply(Yds, Team, mean)
tapply(TD, Team, mean)
# Rushing
tapply(Yds.1, Team, mean)
tapply(TD.1, Team, mean)
# Turnovers
tapply(Fum, Team, mean)
tapply(Int, Team, mean)
tapply(TO, Team, mean)

detach(defense_data)
```

This code looks way too repetitive. How to avoid that? 

That's where fancy coding comes in: **using $tapply()$ within an $apply()$ call**.
We use 'apply' to work on the 'offense_data' data frame's columns, and the function we apply to **each column** is $tapply(..)$. 

```{r}

# Separately create a vector of team names, and of STAT names.
Team.vec <- offense_data$Team
stat.names.off <- c("Yds","TD","Yds.1","TD.1","Fum","Int","Tot.1")
stat.names.def <- c("Yds","TD","Yds.1","TD.1","Fum","Int","TO")

# Offense.

# Means (apply to COLUMNS - MARGIN=2)
off.team.means <- apply(offense_data[,stat.names.off], MARGIN=2, 
                        function(x) tapply(x,Team.vec,mean))
print(off.team.means)

# Medians.
off.team.medians <- apply(offense_data[,stat.names.off], 2, 
                          function(x) tapply(x,Team.vec,median))
print(off.team.medians)

# Defense.
def.team.means <- apply(defense_data[,stat.names.def], 2, 
                        function(x) tapply(x,Team.vec,mean))
print(def.team.means)

def.team.medians <- apply(defense_data[,stat.names.def], 2, 
                          function(x) tapply(x,Team.vec,median))
print(def.team.medians)
```


## Predicting Score Differential via Linear Model.

Now that we have data on both offense and defense, we have a more complete picture for each of 62 games, and we could proceed to try and explain the factors driving the outcome of the game.

```{r}
diff_data <- offense_data
diff_data[,-c(1:5)] <- offense_data[,-c(1:5)] - defense_data[,-c(1:5)]
head(diff_data)
```
Let's take a peek at the fancy correlation matrix that we used before for offensive logs, only now it will be for the  **stat differentials** between offensive and defensive numbers.

```{r}
library(plotrix)

diff_data_num <- diff_data[,-c(1:5)]

# Get thresholded heatmap of correlation matrix. 
abs.cor.mat <- abs(cor(diff_data_num))

# Get the pretty thresholded heatmap of correlation matrix. 
color2D.matplot(ifelse(abs.cor.mat>=0.75, abs.cor.mat,0),
                cs1=c(1,0),cs2=c(1,0),cs3=c(1,0),
                show.legend = T,
                xlab='',
                ylab='',
                axes=F)
par(las=2)
axis(1,at=c(1:ncol(abs.cor.mat))-0.5,labels=colnames(abs.cor.mat))
par(las=1)
axis(2,at=c(ncol(abs.cor.mat):1)-0.5,labels=colnames(abs.cor.mat))
```


Hand-picking the most obvious aspects:

- Completions ($Cmp$) highly correlate with Attempts ($Att$) and Yards ($Yds$).

- The amount of first downs gained via pass or rush ($Pass$, $Rush$) correlate with the volume of passing and rushing plays ($Att$ and $Att.1$), respectively.

- The number of penalties and penalty yardage are correlated.

- Turnover total ($Tot.1$) correlates with both Fumbles ($Fum$) and Interceptions ($Int$), which, in their turn, don't correlate strongly with each other.


So if I were to put together a set of weakly correlated predictors, combining it with domain knowledge for the task in hand, I would:

 - Get rid of all TD stats, as those are in direct effect with the score differential, and it is literally unfair to use them in predicting the score. You can see one strong correlation of Points with Rushing TDs ($TD.1$), for example.
 
 - Get rid of first downs as well, also feels like way too direct of an impact on the score.. plus those correlate heavily with other variables.
 
 From the remaining ones, we could go with: Pass Completions ($Cmp$), Completion Percentage ($Pct$), Rushing Attempts ($Att.1$), Rushing Avgper Attempt ($Avg$) , Overall Yards per Play ($Avg.1$), Number of Penalties ($No.$), Fumbles ($Fum$), Interceptions ($Int$).
 
Let's fit the corresponding linear model of $Points \sim Cmp + Pct + Att.1 + Avg + Avg.1 + No. + Fum + Int$.

```{r}
lm.obj <- lm(Points ~ Cmp + Pct + Att.1 + Avg + Avg.1 + No. + Fum + Int,
             data=diff_data_num)
summary(lm.obj)
```

From the resulting $p$-values, which quantify the significance of each variable with respect to the response (in our case, $Points$):

- The most critical (the smallest $p$-value of $1.91e-06$), to no surprise, is the **average yards per play** ($Avg.1$).

- But right there with it ($p$-value of $7.99e-06$) are **fumbles** ($Fum$).

- Next in line are **rushing attempts** ($Att.1$).

- And then we also see the **number of penalties** ($No.$) making an impact (very small $p$-value of $0.0005$).

Also you may witness that model does a solid job in **explaining the variation in scores** by looking at the $R^2$, which is $0.85 \implies$ this model explains $\approx 85\%$ of variation in score differentials.

Now, all that is concerning **interpretation of the results**, or else known as **inference**. 

How about simply looking at the **predictive performance** of the model? Let's print the 

- **actual** point differential values against 

- those **predicted by our model**.

Predicted values can be obtained via $predict(lm.obj,newdata)$ function, applied to our fitted $lm$ object (which represents our model fit). $newdata$ corresponds to the **explanatory variable values** for which we'd like to **obtain a prediction** of the point differential.
```{r}
# That's actual values
diff_data$Points
# That's how we obtain predicted values, via predict() function
model.vars <- c("Cmp","Pct","Att.1","Avg","Avg.1","No.","Fum","Int")
predict(lm.obj,newdata = diff_data_num[,model.vars])

```

To get a better feel of how the predictions actually match up with the true values, we combine them into a data frame, in column-by-column fashion.

```{r}
pred.mat <- data.frame(true = diff_data_num$Points,
                       predicted = predict(lm.obj,diff_data_num[,model.vars]))
head(pred.mat,20)
print(sum((pred.mat$true - pred.mat$predicted)^2))
```
Want to make the predicted values look a bit nicer? Round them up to an integer via $round()$ function:

```{r}
pred.mat <- data.frame(true = diff_data_num$Points,
                       predicted = predict(lm.obj,diff_data_num[,model.vars]))
pred.mat$predicted <- round(pred.mat$predicted)
head(pred.mat,20)
```


To plot all the residuals ($\hat{y}_i - y_i$) of our model's predictions $\hat{y}_i$:
```{r}
plot(residuals(lm.obj))
```

While there's plenty of **point predictions** that aren't that close to the true values, we could obtain **prediction intervals** which have a much better chance of containing the actual values.

While **point predictions** address an **average expected response** for **these values of explanatory variables**, the **prediction intervals** account for natural variability and try to include 95\% of all future observed responses for **these values of explanatory variables**. They are also referred to as **wide intervals**, and you will see why:

```{r}
pred.int <- predict(lm.obj,newdata=diff_data_num[,model.vars], int="predict")
head(pred.int,10)

pred.mat <- data.frame(true = diff_data_num$Points,
                       predicted.low = round(pred.int[,"lwr"]),
                       predicted.high = round(pred.int[,"upr"]))
head(pred.mat, 20)
```
The **confidence intervals**, or **narrow intervals**, correspond to uncertainty around an **average prediction** for this set of predictor values. So they show an interval for which the **average of future observations** for **this set of predictor values** will land with **95\% confidence**.

```{r}
pred.int <- predict(lm.obj,newdata=diff_data_num[,model.vars], int="conf")
head(pred.int,10)

pred.mat <- data.frame(true = diff_data_num$Points,
                       predicted.low = round(pred.int[,"lwr"]),
                       predicted.high = round(pred.int[,"upr"]))
head(pred.mat, 20)
```


## Train/Test Subset Approach.

More proper and logical approach is to test the prediction performance of the model on the data that **was NOT used for training**. To achieve this, we break the original data set into a

- **training** subset, to **fit** the model, and

- **test** subset, to **test the prediction performance** of the model.

Below we proceed to randomly pick a subset of original data (90\% of observations) to be used for training, while the remaining 10\% as a holdout test data, to evaluate prediction accuracy.

```{r}
set.seed(1)
n <- nrow(diff_data_num)
train.size <- 0.9*nrow(diff_data_num)
train.ind <- sample(1:n,train.size)
test.ind <- c(1:n)[-train.ind]

model.vars <- c("Cmp","Pct","Att.1","Avg","Avg.1","No.","Fum","Int")

lm.obj <- lm(Points ~ Cmp + Pct + Att.1 + Avg + Avg.1 + No. + Fum + Int,
             data=diff_data[train.ind,])

pred.mat <- data.frame(true = diff_data_num[test.ind,]$Points,
                       predicted = predict(lm.obj, diff_data_num[test.ind,model.vars]))
pred.mat$predicted <- round(pred.mat$predicted)
head(pred.mat,20)

MSE <- sum((pred.mat$true - pred.mat$predicted)^2)/nrow(pred.mat)
MSE
```


Now, we could do it for many random subdivisions of original data into training and testing subsets, to obtain a **better, more reliable, estimate** of a **true prediction error rate**:

```{r}
B <- 100
all.MSEs <- numeric(B)

for (b in 1:B){
  n <- nrow(diff_data_num)
  train.size <- 0.9*nrow(diff_data_num)
  train.ind <- sample(1:n,train.size)
  test.ind <- c(1:n)[-train.ind]
  
  model.vars <- c("Cmp","Pct","Att.1","Avg","Avg.1","No.","Fum","Int")

  lm.obj <- lm(Points ~ Cmp + Pct + Att.1 + Avg + Avg.1 + No. + Fum + Int,
               data=diff_data[train.ind,])

  pred.mat <- data.frame(true = diff_data[test.ind,]$Points,
                         predicted = predict(lm.obj, diff_data[test.ind,model.vars]))

  all.MSEs[b] <- sum((pred.mat$true - pred.mat$predicted)^2)/nrow(pred.mat)
}

print(mean(all.MSEs))
hist(all.MSEs)
```

